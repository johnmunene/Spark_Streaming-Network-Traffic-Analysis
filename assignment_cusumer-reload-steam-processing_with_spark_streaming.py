# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4Wp2fqYhOo9sPe7jkHZkcfkpSWmPSOt
"""

!pip install confluent-kafka

!pip install pyspark==3.1.1

# We set the environment variable PYSPARK_SUBMIT_ARGS to include the necessary
# package dependency for Kafka integration in the Spark application.
import os
os.environ["PYSPARK_SUBMIT_ARGS"] = "--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 pyspark-shell"

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, to_json
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
import os
import json

# Create a SparkSession
spark = SparkSession.builder.appName("KafkaSparkStructuredStreamingExample").getOrCreate()

# Define the Kafka topic schema
schema = StructType([
    StructField("source_ip", DoubleType()),
    StructField("destination_ip", DoubleType()),
    StructField("bytes_sent", DoubleType())
])

# Read data from Kafka topic as a streaming DataFrame
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "pkc-6ojv2.us-west4.gcp.confluent.cloud:9092") \
    .option("kafka.security.protocol", "SASL_SSL") \
    .option("kafka.sasl.mechanism", "PLAIN") \
    .option("kafka.sasl.username", "FLFB6XWSLZEKDDAB") \
    .option("kafka.sasl.password", "YTBPyutYDrqzcCSW11sq14GxNWEjd3TBBRR40cJYsJgpkNoEZ0kZMvK/ZlIhn765") \
    .option("subscribe", "network-traffic") \
    .load()

# Parse the JSON data from Kafka topic
parsed_df = df.selectExpr("CAST(value AS STRING)") \
    .select(from_json("value", schema).alias("data")) \
    .select("data.*")

# Perform data processing on the parsed DataFrame
processed_df = parsed_df.select("source_ip", "destination_ip", "bytes_sent")
# You can apply various transformations and actions on `processed_df` as per your requirements

# Specify the checkpoint directory path
checkpoint_dir = "/tmp/checkpoints"  # Replace with your desired checkpoint directory

# Create the checkpoint directory if it doesn't exist
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

# Write the processed data to Kafka topic
query = processed_df \
    .selectExpr("CAST(source_ip AS STRING) AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "pkc-6ojv2.us-west4.gcp.confluent.cloud:9092") \
    .option("kafka.security.protocol", "SASL_SSL") \
    .option("kafka.sasl.mechanism", "PLAIN") \
    .option("kafka.sasl.username", "FLFB6XWSLZEKDDAB") \
    .option("kafka.sasl.password", "YTBPyutYDrqzcCSW11sq14GxNWEjd3TBBRR40cJYsJgpkNoEZ0kZMvK/ZlIhn765") \
    .option("topic", "processed-data") \
    .outputMode("update") \
    .option("checkpointLocation", checkpoint_dir) \
    .start()

# Wait for the query to terminate
query.awaitTermination()