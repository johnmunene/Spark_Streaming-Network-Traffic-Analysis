# -*- coding: utf-8 -*-
"""Assignment-producer: Basic Stream Processing with Spark Streaming

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q5jOVoNPunuVkTT53Vjssmu46xe9cXvJ

# Sample Starter Code: Basic Stream Processing with Spark Streaming

Here's some sample code in Python using the kafka-python package for generating network traffic data and publishing it to the Kafka topic.
"""

!pip install confluent_kafka

!pip install pyspark==3.1.1

# We set the environment variable PYSPARK_SUBMIT_ARGS to include the necessary
# package dependency for Kafka integration in the Spark application.
import os
os.environ["PYSPARK_SUBMIT_ARGS"] = "--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 pyspark-shell"

from confluent_kafka import Producer
import time
import json
import random

conf = {
    'bootstrap.servers': 'pkc-lzvrd.us-west4.gcp.confluent.cloud:9092',
    'security.protocol': 'SASL_SSL',
    'sasl.mechanism': 'PLAIN',
    'sasl.username': 'FLFB6XWSLZEKDDAB',
    'sasl.password': 'YTBPyutYDrqzcCSW11sq14GxNWEjd3TBBRR40cJYsJgpkNoEZ0kZMvK/ZlIhn765'

}

producer = Producer(conf)

while True:
    # generate random data for the stream
    data = {
        "source_ip": '.'.join(str(random.randint(0, 255)) for _ in range(4)),
        "destination_ip": '.'.join(str(random.randint(0, 255)) for _ in range(4)),
        "bytes_sent": random.randint(1000, 100000)
    }

    # convert data to JSON string and send to Kafka topic
    producer.produce('network-traffic', json.dumps(data).encode('utf-8'))
    producer.flush()
    # Wait for 1 second before generating next network traffic data
    time.sleep(1)